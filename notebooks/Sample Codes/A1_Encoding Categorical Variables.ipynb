{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a10bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sample only introduces two commonly used encoders, \n",
    "# there are many for categorical variables.\n",
    "# We may cover more when we talk about features engineering.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "# Sizes\n",
    "n = 1000\n",
    "\n",
    "# Simulate the non-ordinal categorical variable X1 with 3 types\n",
    "X1 = np.random.choice(['Type1', 'Type2', 'Type3'], size=n)\n",
    "\n",
    "# Simulate the ordinal categorical variable X2 with 3 types\n",
    "X2 = np.random.choice(['Low', 'Medium', 'High'], size=n, p=[0.3, 0.4, 0.3])\n",
    "\n",
    "# Simulate the target variable Y based on X1 and X2\n",
    "# For simplicity, using a simple interaction model here. Adjust as needed.\n",
    "Y = np.array([np.random.normal(0, 1) + (1 if x1 == 'Type2' else 0) + \n",
    "              (0.5 if x2 == 'Medium' else 1 if x2 == 'High' else -0.5) for x1, x2 in zip(X1, X2)])\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'X1': X1, 'X2': X2, 'Y': Y})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "442af9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Encoder for X1 (non-ordinal)\n",
    "encoder_X1 = OneHotEncoder(sparse=False)\n",
    "X1_encoded = encoder_X1.fit_transform(df[['X1']])\n",
    "\n",
    "# Encoder for X2 (ordinal), see the bottom for the difference of two encoders.\n",
    "encoder_X2 = LabelEncoder()\n",
    "df['X2_encoded'] = encoder_X2.fit_transform(df['X2'])\n",
    "\n",
    "# Combine X1_encoded and X2_encoded\n",
    "X = np.hstack((X1_encoded, df[['X2_encoded']]))\n",
    "\n",
    "# Target variable\n",
    "Y = df['Y'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d1873a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "RMSE: 1.0162537818095863\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = LGBMRegressor()\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "Y_pred = grid_search.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
    "\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "print(f'RMSE: {rmse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d0311",
   "metadata": {},
   "source": [
    "# Categorical Encoders in Python\n",
    "\n",
    "Categorical data are common in data science and can represent types of categories, like countries, colors, brands, etc. Handling categorical data is not always straightforward because most machine learning algorithms require numerical input. Therefore, encoding categorical variables is a crucial step. Below are several methods for encoding categorical variables in Python:\n",
    "\n",
    "## 1. Label Encoding\n",
    "\n",
    "Label Encoding is a simple and straightforward method. It involves converting each value in a column to a number.\n",
    "\n",
    "- **Use Case**: It is useful with ordinal data (e.g., low, medium, high).\n",
    "- **Scikit-learn Implementation**: `LabelEncoder`\n",
    "- **Limitations**: Implies ordinality when there is none, and can lead to misinterpretation by algorithms.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example\n",
    "label_encoder = LabelEncoder()\n",
    "data['Category'] = label_encoder.fit_transform(data['Category'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479f2dd0",
   "metadata": {},
   "source": [
    "# Differences between LabelEncoder and OrdinalEncoder\n",
    "\n",
    "When dealing with categorical data in machine learning, it's crucial to encode categorical variables properly. `LabelEncoder` and `OrdinalEncoder` are two common encoders provided by the scikit-learn library in Python. Though they might seem similar at first glance, they serve different purposes and are used in different scenarios.\n",
    "\n",
    "## LabelEncoder\n",
    "\n",
    "`LabelEncoder` is a utility class to help normalize labels such that they contain only values between 0 and `n_classes-1`. It's used to transform non-numerical labels to numerical labels (or nominal categorical variables).\n",
    "\n",
    "### Key Characteristics of LabelEncoder:\n",
    "- It encodes labels with a value between 0 and `n_classes-1`.\n",
    "- It's used for encoding target values (y), i.e., the response variable, not the input (X) variables.\n",
    "- It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.\n",
    "\n",
    "### Usage:\n",
    "Typically, `LabelEncoder` is used in scenarios where the categorical labels are used for encoding target values (y). For instance, in classification problems where you have labels like \"spam\" and \"not spam\".\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "data['Target'] = label_encoder.fit_transform(data['Target'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b53f33b",
   "metadata": {},
   "source": [
    "## OrdinalEncoder\n",
    "\n",
    "`OrdinalEncoder` is used to convert categorical features to ordinal, and it is specifically designed for feature encoding (X) rather than target variable encoding (y).\n",
    "\n",
    "### Key Characteristics of OrdinalEncoder:\n",
    "- **Encodes categorical features as an integer array**: The encoder converts the data into a format that is interpretable by the machine learning model, assigning each unique category in the feature column an integer value.\n",
    "- **Meant for feature encoding (X)**: Unlike `LabelEncoder` which is often used for the target variable (y), `OrdinalEncoder` is designed to preprocess the input features (X).\n",
    "- **Handles 2D data**: The input to this transformer should be a 2D array-like structure representing multiple features.\n",
    "\n",
    "### Usage:\n",
    "`OrdinalEncoder` is particularly useful for encoding categorical features where an ordinal relationship exists and the order of the categories is important. This is commonly seen in ordinal categorical variables such as ratings (e.g., \"bad\", \"average\", \"good\").\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Suppose 'Feature' is an ordinal categorical column in the DataFrame 'data'\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "data_encoded = ordinal_encoder.fit_transform(data[['Feature']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e7f350",
   "metadata": {},
   "source": [
    "## Comparison between LabelEncoder and OrdinalEncoder\n",
    "\n",
    "| Aspect                  | LabelEncoder                               | OrdinalEncoder                          |\n",
    "|-------------------------|--------------------------------------------|-----------------------------------------|\n",
    "| Use Case                | Encoding target variables (y)              | Encoding input features (X)             |\n",
    "| Data Type               | 1D array                                   | 2D array                                |\n",
    "| Encoded Output          | 1D array                                   | 2D array                                |\n",
    "| Suitable for            | Nominal Categorical Variables              | Ordinal Categorical Variables           |\n",
    "| Relationship Imposed    | Ordinal (though often used for Nominal)    | Ordinal                                 |\n",
    "\n",
    "In summary, while both `LabelEncoder` and `OrdinalEncoder` can be used to transform categorical variables into numbers, they are intended for different types of data and serve different purposes in a machine learning workflow. `LabelEncoder` is primarily for the target variable, and `OrdinalEncoder` is for input features, especially when those features are ordinal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972cc8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
